## 文本预处理

### 文本预处理步骤：

1. 读入文本
2. 分词:对每个句子进行分词，也就是将一个句子划分成若干个词（token），转换为一个词的序列。
3. 建立字典，将每个词映射到一个唯一的索引（index）
4. 将文本从词的序列转换为索引的序列，方便输入模型

### 相关概念

> **pad**的作用是在采用批量样本训练时，对于长度不同的样本（句子），对于短的样本采用pad进行填充，使得每个样本的长度是一致的
> **bos**( begin of sentence)和**eos**(end of sentence)是用来表示一句话的开始和结尾
> **unk**(unknow)的作用是，处理遇到从未出现在预料库的词时都统一认为是unknow ,在代码中还可以将一些频率特别低的词也归为这一类

### 分词工具

```python
#spaCy:
import spacy
nlp = spacy.load('en_core_web_sm')
doc = nlp(text)

#NLTK:
from nltk.tokenize import word_tokenize
print(word_tokenize(text))
```

## 语言模型

### 概念

一段自然语言文本可以看作是一个离散时间序列，给定一个长度为T的词的序列，语言模型的目标就是评估该序列是否合理，即计算该序列的概率，语言模型的参数就是词的概率以及给定前几个词情况下的条件概率

### n元语法

序列长度增加，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。n元语法通过马尔可夫假设简化模型，马尔科夫假设是指一个词的出现只与前面n个词相关，即n阶马尔可夫链（Markov chain of order n）
以上也叫n元语法（n-grams），它是基于n - 1阶马尔可夫链的概率语言模型。
当n分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法

> n元语法可能有哪些缺陷？
>
> 1. 参数空间过大
> 2. 数据稀疏

### 时序数据的采样

#### 随机采样

每次从数据里随机采样一个小批量。其中批量大小`batch_size`是每个小批量的样本数，`num_steps`是每个样本所包含的时间步数。在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。

#### 相邻采样

在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻。

## 循环神经网络基础

### 循环神经网络的构造

![Image Name](https://cdn.kesci.com/upload/image/q5jkm0v44i.png?imageView2/0/w/640/h/640)

由于引入了隐藏变量H，能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。

### 裁剪梯度

循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。

裁剪梯度（clip gradient）是一种应对`梯度爆炸`的方法。

假设我们把所有模型参数的梯度拼接成一个向量 ，裁剪后的梯度的范数不超过阈值。

### 困惑度

我们通常使用困惑度（perplexity）来评价语言模型的好坏。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，

- 最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；

- 最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；

- 基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。

  > 任何一个有效模型的困惑度必须小于类别个数。

### 不同采样方式下的参数初始化

**随机采样**：每次迭代`都需要`重新初始化隐藏状态

（每个epoch的每次迭代都需要进行初始化，因为对于随机采样的样本中只有一个批量内的数据是连续的）
**相邻采样**：如果是相邻采样，前后两个batch的数据是连续的

（每个batch的只更新一次，模型在一个epoch中的迭代`不需要`重新初始化隐藏状态） ﻿

### pytorch函数

> detach()和_detach()区别
> .detach()：从当前计算图中分离下来的，但是仍指向原变量的存放位置，所以计算图不变。
> ._detach()：将一个Variable从创建它的图中分离，并把它设置成叶子 variable，这里计算图就改变了。
>
> torch.cat()
> C = torch.cat( (A,B),0 ) #按维数0拼接（竖着拼）
> C = torch.cat( (A,B),1 ) #按维数1拼接（横着拼）
>
> RNN层
> nn.RNN(input_size, hidden_size)
>
> tensor的维度的元素叠加
> torch.stack()
>
> adam优化方式
> torch.optim.Adam(model.parameters(), lr=lr)
