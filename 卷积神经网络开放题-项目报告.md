## 问题一：LeNet v.s. AlexNet

#### 阅读上述两种网络的相关论文，试从`数据集的预处理`、`激活函数的使用`、`训练方法的改进`以及`模型结构的变化`等角度，从**理论层面**分析比较LeNet与AlexNet的结构差异，并尝试解释AlexNet为什么会具有对计算机视觉任务优越的处理性能。

![Image Name](https://cdn.kesci.com/upload/image/q5kv4gpx88.png?imageView2/0/w/640/h/640)

|                | LeNet                        | AlexNet                              |
| -------------- | ---------------------------- | ------------------------------------ |
| 数据集         | 手写数字集（28*28，10类）    | ImageNet数据集（3\*224*224，1000类） |
| 数据集的预处理 | 无特殊的预处理               | 数据增强*                            |
| 激活函数的使用 | Sigmoid/tanh（后者收敛更快） | ReLU*                                |
| 训练方法       | 小批量随机梯度下降           | 小批量随机梯度下降+Dropout           |
| 模型结构       | 2个卷积层+3个全连接层*       | `5`个卷积层+3个全连接层*             |



**数据增强**

为了**降低过拟合**，提高模型的鲁棒性，AlexNet采用了两种数据增强(Data Augmentation)方式：

- 图像平移与水平反射（镜像）

![img](https://cdn.nlark.com/yuque/0/2019/png/353587/1566272569472-b9a11da2-25cb-4a9c-8922-7cac0c1f5668.png)

测试时，网络剪裁5个224×224图块（4个角图块和1个中心图块）以及它们的水平反射（总共10个）进行预测，并对网络的softmax层的预测求10个图块平均值 。

- By image translation（图像平移）: (256– 224)² = 32² = 1024

- By horizontal reflection（水平反射）: 1024 × 2 = 2048

- 改变训练集RGB通道的图像像素强度(intensity)

  ![img](https://cdn.nlark.com/yuque/0/2019/png/353587/1566272680651-1f689591-20d0-48bb-b27f-c4d6aa83457d.png)

**非线性ReLU**

- 使用ReLU 作为CNN 的激活函数，并验证其效果在较深的网络超过了Sigmoid，解决了Sigmoid在网络较深时的梯度弥散问题，提高了网络的训练速率

**LeNet网络结构**

- 关于LeNet的网络结构也有3个卷积层+2个全连接层的说法

  ![img](https://cdn.nlark.com/yuque/0/2019/png/653487/1576200015803-7d48ad06-b845-4b95-944d-05ef7e1fd00b.png#align=left&display=inline&height=212&originHeight=594&originWidth=2064&size=0&status=done&style=none&width=735)

  这种分发源自于上图中的`C5`，既可以看作是`卷积层`也可以看作是`全连接层`（因为上一层得到的特征图是5x5，卷积核也为5x5，该层参数：5×5×16×120+120 = 48120） 

**AlexNet网络结构**

- AlexNet在LeNet基础上进行了更宽更深的网络设计

  ![img](https://cdn.nlark.com/yuque/0/2019/png/219582/1560759120175-5b8a9a7f-27de-4036-92a2-a0489c43ee00.png)

  AlexNet网络采用：5个卷积层+3个全连接层

  在论文原文中，还提到了的

  - 重叠最大池化(Overlapping max-pooling)
  - 局部响应归一化 Local Response Normalization (LRN）

  但实际上并没有取得特别好的效果，所以，这里就不展开讲了，感兴趣的小伙伴可自行参阅原文了解

**AlexNet为什么会具有对计算机视觉任务优越的处理性能？**

认识事物要从本质入手

完成一个任务，可以使用不同的方法/工具

在本问题中，需要解决的任务是CV，采用的是AlexNet，即深度学习(深度学习的本质是表征学习)

这里AlexNet的使用，确切的说，是能解决好，CV中的分类任务

AlexNet这一模型的之所以能很好的解决视觉分类任务，我认为这是机器模仿人类学习的结果

AlexNet模型中最重要的模块就是`卷积层`，卷积层的工作机制就是在模仿人类处理视觉任务的过程，随着层数的增加，特征结构更加复杂，学习到更加高级的特征，越来越接近我们要识别的目标

可能有人会问，“就一个卷积层就能让机器在执行视觉分类任务时，远超人类嘛？我感觉，人的视觉系统似乎更厉害些啊？”

是的，人在处理视觉相关问题的过程远比AlexNet模型更加高明，比如，人可以凭借已有的知识来辅助分类，但大家也不要忘记了，深度学习的崛起，除了有更好的模型，另外一个因素就是更多的数据

| <img src="https://c-ssl.duitang.com/uploads/item/201508/18/20150818140314_Nmdfv.jpeg" alt="可爱 萌宠 宠物 动物 卖萌让专业的来 小老虎" style="zoom:50%;" /> | <img src="http://www.sinaimg.cn/dy/w/2007-10-18/54ee190f80c49efa60d39680c3470f69.jpg" alt="“虎皮猫”新丁驾到" style="zoom: 67%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
|                            小老虎                            |                            虎皮猫                            |

人力有穷，而计算机却能做到不知疲倦。有时候笨办法就是最好的办法，我只要喂给模型足够多图片，让模型去学习，机器自然能做到比人更好。

我们做科研的人总是在思考有没有更好方法去解决同一问题，比如，人是如何从真实图片学习到的猫狗分类，然后将这种分类能力运用到卡通图片的分类中的；数据量不足的时候，如何让模型也能有更好的分类效果……

#### AlexNet对Fashion-MNIST数据集来说可能过于复杂，请尝试对`模型进行简化`来使训练更快，同时保证分类准确率（accuracy）不明显下降（不低于85%），并将简化后的结构、节省的训练时间以及下降的准确率等相关指标以表格的形式进行总结分析。

| 模型结构or超参数                                | 平均训练时间 | 测试集准确率 |
| ----------------------------------------------- | ------------ | ------------ |
| 修改全连接输出层：1000→10；lr：0.001；epochs：3 | 265.6 s      | 0.863        |
| 删除参数最多的全连接隐藏层：1层                 | 213.5 s      | 0.887        |
| 删除第4层卷积层                                 | 190.9 s      | 0.886        |
| 增大batch_size：16→128                          | 81.8 s       | 0.903        |
| 增大学习率lr：0.001→0.003                       | 80.6 s       | 0.893        |
| 减小学习率lr：0.003→`0.002`                     | `80.3 s`     | `0.908`      |
| 增大batch_size：128→256                         | 73.7 s       | 0.894        |

## 问题二：

#### LeNet与AlexNet在接近图像输入的卷积模块中都引入了较大尺寸的卷积核，如5×5或者7×7的卷积窗口来捕捉更大范围的图像信息，试`分析`VGG每个基础块的固定设计是否会影响到图像的粗粒度信息提取，并且`对比`不同结构模块输出的特征图进行对比。

**每个VGG基础块的固定设计对图像的粗粒度信息提取的影响**

我对图像粗粒度信息提取的看法是，其关键在于感受野的大小。在接近图像输入的卷积模块中都引入较大尺寸的卷积核，与VGG基础块中使用多个3*3卷积核代替一个较大尺寸的卷积核，其在感受野的大小上是等效的，应该`不会`影响到图像的粗粒度信息提取。

![评论图片](https://staticcdn.boyuai.com/comment/upload/TpvcxvzPDLi2gExytiDBV/3006/2020/02/23/jOOyFY3M2AvbfeB42_6v8.jpg)

从特征提取的角度来看，采用连续`三`个3*3卷积核替代`一`个7\*7卷积核，多经过了`二`次非线性的激活函数，模型实际上提取的特征层次更加丰富了

从模型训练角度来看，7\*7卷积参数为：49个；3个3*3卷积参数为：27个，相差近一倍，参数少了训练时间自然自然也会相应的缩短。

**不同结构模块输出的特征图进行对比**

VGG每个基础块组成规律是：

- 连续使用数个相同的填充为1(SAME padding)、卷积核大小为3×3的卷积层，后接上一个步幅为2、池化核为2×2的最大池化层。
- 卷积层保持输入的高和宽不变，而池化层则对其减半。

![img](https://cdn.nlark.com/yuque/0/2019/png/219582/1560994342276-e8428f6a-f2fa-4b21-9248-0ed5b36d8fa8.png)



VGGNet的卷积层有一个显著的特点：

- **特征图的空间分辨率单调`递减`**，**特征图的通道数单调`递增`**。这使得输入图像在维度上流畅地转换到分类向量。
- AlexNet的通道数无此规律，VGGNet后续的GoogLeNet和ResNet均遵循此维度变化的规律。



#### 尝试将Fashion-MNIST中图像的高和宽由224改为96，试`分析`VGG网络的参数变化情况，并且`对比`模型训练时间、分类准确率（accuracy）等实验指标受到的影响，以表格的形式进行总结分析。

首先，VGGNet本事就是为ImageNet数据集设计的网络，该数据集的图片样本大小为224\*224*3，网络的主要参数集中在全连接层上。

而在本实验中数据集替换为Fashion-MNIST数据集，该数据集的图片样本大小为28\*28，因此，我们在载入数据时，使用了`torchvision.transforms.Resize(size=resize)`根据需要，变换了输入网络的图片形状大小。

另外还需注意的是，本实验中，数据集并不大，所以网络层数不深，使用的是`VGG11`模型。改变实验变量输入图像的大小由`224→96`，在VGGNet中参数的变化主要集中在`全连接第一层`中。

| input_size | 224×224（未计算偏置项）              | 96×96（未计算偏置项）               |
| ---------- | ------------------------------------ | ----------------------------------- |
| input      | 图像：224×224×1    参数：0           | 图像：96×96×1    参数：0            |
| conv1-1    | 图像：224×224×64 参数：(3×3×1)×64    | 图像：96×96×64    参数：(3×3×1)×64  |
| pool       | 图像：112×112×64 参数：0             | 图像：48×48×64 参数：0              |
| conv2-1    | 图像：112×112×128 参数：(3×3×64)×128 | 图像：48×48×128 参数：(3×3×64)×128  |
| pool       | 图像：56×56×128 参数：0              | 图像：24×24×128 参数：0             |
| conv3-1    | 图像：56×56×256 参数：(3×3×128)×256  | 图像：24×24×256 参数：(3×3×128)×256 |
| conv3-2    | 图像：56×56×256 参数：(3×3×256)×256  | 图像：24×24×256 参数：(3×3×256)×256 |
| pool       | 图像：28×28×256 参数：0              | 图像：12×12×256 参数：0             |
| conv4-1    | 图像：28×28×512 参数：(3×3×256)×512  | 图像：12×12×512 参数：(3×3×256)×512 |
| conv4-2    | 图像：28×28×512 参数：(3×3×512)×512  | 图像：12×12×512 参数：(3×3×512)×512 |
| pool       | 图像：14×14×512 参数：0              | 图像：6×6×512 参数：0               |
| conv5-1    | 图像：14×14×512 参数：(3×3×512)×512  | 图像：6×6×512 参数：(3×3×512)×512   |
| conv5-2    | 图像：14×14×512 参数：(3×3×512)×512  | 图像：6×6×512 参数：(3×3×512)×512   |
| pool       | 图像：7×7×512 参数：0                | 图像：3×3×512 参数：0               |
| FC 4096    | 图像：1×1×4096 参数：`7×7×512×4096`  | 图像：1×1×4096 参数：`3×3×512×4096` |
| FC 4096    | 图像：1×1×4096 参数：4096×4096       | 图像：1×1×4096 参数：4096×4096      |
| FC 10      | 图像：1×1×10 参数：4096×10           | 图像：1×1×10 参数：4096×10          |

下面这张表格比较了输入图像大小由224→96时，训练时间、分类准确率的变化（lr, num_epochs = 0.001, 5）

| 输入图像大小 | 224*224 | 96*96  |
| ------------ | ------- | ------ |
| 平均训练时间 | 96.7 s  | 25.5 s |
| 分类准确率   | 0.911   | 0.912  |

在分析过VGGNet自身的结构和改变输入图像大小过后，我们发现：

- 模型参数主要集中在全连接层，而从网络输入端减小输入图片的大小，能很大程度上减小模型参数，这使得模型的训练时间显著缩短
- 同时我们也发现，模型的分类准确率仍能得到较好的保留

## 问题三：

#### 对比AlexNet、VGG和`NiN`、`GoogLeNet`的`模型参数尺寸`，从理论的层面分析为什么`后两个`网络可以显著减小模型参数尺寸？

#### AlexNet

|                           模型尺寸                           |                         模型参数大小                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
| ![img](https://cdn.nlark.com/yuque/0/2020/png/478234/1579761518340-e4d9e61e-0876-4dd0-8876-aca1a55137aa.png) | ![img](https://cdn.nlark.com/yuque/0/2020/png/478234/1579761453080-e885ac08-fe15-4d92-8119-9023e101daf2.png) |

**1.网络结构**：如下图所示，8层网络，参数大约有60 million，使用了relu函数，头两个全连接层使用了0.5的dropout。使用了LRN和重叠的池化，现在LRN都不用了，一般用BN作Normalization。当时使用了多GPU训练。

**2.预处理**：先down-sample成最短边为256的图像，然后剪出中间的256x256图像，再减均值做归一化（over training set）。 **训练时，做数据增强**，对每张图像，随机提取出227x227以及水平镜像版本的图像。除了数据增强，还使用了PCA对RGB像素降维的方式来缓和过拟合问题。

**3.预测**：对每张图像提取出5张（四个角落以及中间）以及水平镜像版本，总共10张，平均10个预测作为最终预测。

**4.超参数**：SGD，学习率0.01，batch size是128，momentum为0.9，weight decay为0.0005（论文有个权重更新公式），每当validation error不再下降时，学习率除以10。权重初始化用（0，0.01）的高斯分布，二四五卷积层和全连接层的bias初始化为1（给relu提供正值利于加速前期训练），其余bias初始化为0。

#### VGG

|                            VGG16                             |
| :----------------------------------------------------------: |
| ![img](https://cdn.nlark.com/yuque/0/2020/png/478234/1579761454091-b4e32034-9254-4ab3-b4d4-a721efbc941b.png) |

从网络模型可以看出，VGG16相比AlexNet类的模型具有较深的深度，通过反复堆叠`3*3`的卷积层和`2*2`的池化层，VGG16构建了较深层次的网络结构，整个网络的卷积核使用了一致的`3*3`的尺寸，最大池化层尺寸也一致为`2*2`。

与AlexNet主要有以下不同：

- Vgg16有16层网络，AlexNet只有8层；
- 在训练和测试时使用了多尺度做数据增强。

**1.感受野**

对于给定的感受野，采用堆积的小卷积核优于采用大的卷积核,因为可以增加网络深度，来保证学习更复杂的模式，而且代价还比较小(参数更少)。

VGG中，使用3个3x3卷积核来代替7x7卷积核,使用了2个3x3卷积核来代替5*5卷积核,这样做的主要目的是在保证具有相同感受野的条件下,提升了网络的深度,在一定程度上提升了神经网络的效果。

此外VGG使用相同大小的卷积核，还利于模块化设计，方便代码实现。

**2.模型参数**

VGG层之间高和宽减半、通道数翻倍的设计使得多数卷积层都有相同的模型参数尺寸和计算复杂度。由于每个卷积层的窗口大小一样，所以每层的模型参数尺寸和计算复杂度与输入高、输入宽。输入通道数和输出通道数的乘积成正比。

#### NiN

|  左图展示了AlexNet和VGG的网络结构，右图展示了NiN的网络结构   |
| :----------------------------------------------------------: |
| ![img](https://cdn.nlark.com/yuque/0/2020/svg/478234/1579761453691-360a0f25-ba0b-4803-a0e6-c2c8d206dc2c.svg) |

|   左图是AlexNet和VGG的网络结构局部,右图是NiN的网络结构局部   |
| :----------------------------------------------------------: |
| <img src="https://cdn.nlark.com/yuque/0/2020/png/478234/1579761548874-697b9731-6f0f-48a5-8f4d-22e5198d78aa.png" alt="img" style="zoom:67%;" /> |

**1.局部快**

NiN块是NiN中的基础块。它由一个卷积层加两个充当全连接层的卷积层串联而成。

其中第一个卷积层的超参数可以自行设置,而而第二和第三个卷积层的超参数一般是固定的。

如果要堆叠小模型，那么就无法避免全连接层，而将卷积结果的特征图转为全连接的维度转换，将耗费大量的资源。而使用1x1的卷积层正好可以完成这一替换。

1x1的卷积层，可以看成全连接层，其中空间维度（高和宽）上的每个元素相当于样本，通道相当于特征。因此，NIN使用1x1卷积层来替代全连接层，从而使空间信息能够自然传递到后面的层。

**2.全局平均池化**

NiN还有一个设计与AlexNet显著不不同:

NiN去掉了了AlexNet最后的3个全连接层,取而代之地,NiN使用用了输出通道数等于标签类别数的NiN块,然后使用全局平均池化层对每个通道中所有元素求平均并直接用于分类。

这里的全局平均池化层即窗口形状等于输入空间维形状的平均池化层。

NiN的这个设计的好处是可以显著减小模型参数尺寸,从而缓解过拟合。然而,该设计有时会造成获得有效模型的训练时间的增加。

#### GoogLeNet

|                      完整GoogLeNet模型                       |
| :----------------------------------------------------------: |
| ![Image Name](https://cdn.kesci.com/upload/image/q5l6x0fyyn.png?imageView2/0/w/640/h/640) |
|                       **Inception块**                        |
| ![img](https://cdn.nlark.com/yuque/0/2019/svg/219582/1556504262639-be33bd10-2404-4855-b42c-d599030588f8.svg) |

**1.Inception块结构**

该块采用四条线路提取不同空间尺度下的信息，其中1x1卷计层用来减少输入通道数或调整输出通道数，以降低模型复杂度。4条线路都使用了合适的填充来使输入与输出的高和宽一致。

**2.完整网络参数**

![arch.png](https://cdn.nlark.com/yuque/0/2020/png/478234/1579761608316-e0b2c7b7-8098-4fe5-bf9d-5341edb87006.png)

- Inception块相当于一个有4条线路的子网络。它通过不同窗口形状的卷积层和最大池化层来并行抽取信息,并使用1x1卷积层减少通道数从而而降低模型复杂度。

- GoogLeNet将多个设计精细的Inception块和其他层串联起来。其中Inception块的通道数分配之比是在ImageNet数据集上通过大量的实验得来的。

- GoogLeNet和它的后继者们一度是ImageNet上最高效的模型之一:在类似的测试精度下,它们的计算复杂度往往更低。

#### 分析上述网络，不难发现NiN、GoogLeNet可以显著减小模型参数尺寸的主要原因在于使用了1×1卷积代替了FC，很大程度上减少了模型参数，防止了过拟合的发生。

这里将1×1卷积的作用总结如下：

1.放缩通道数：通过控制卷积核的数量达到通道数的放缩。
2.增加非线性：1×1卷积核的卷积过程相当于全连接层的计算过程，并且还加入了非线性激活函数，从而可以增加网络的非线性。
3.计算参数少

#### GoogLeNet有数个后续版本，包括加入批量归一化层 [7]、对Inception块做调整 [8] 和加入残差连接 [9]，请尝试实现并运行它们，然后观察实验结果，以表格的形式进行总结分析。

[7] Ioffe, S., & Szegedy, C. (2015). Batch normalization: Accelerating deep network training by reducing internal covariate shift. arXiv preprint arXiv:1502.03167.

[8] Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., & Wojna, Z. (2016). Rethinking the inception architecture for computer vision. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2818-2826).

[9] Szegedy, C., Ioffe, S., Vanhoucke, V., & Alemi, A. A. (2017, February). Inception-v4, inception-resnet and the impact of residual connections on learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 4, p. 12).

这里仅从理论层面简要分析了GoogleNet家族的模型结构

| V1版本提出了Inception的概念，大胆使用了1*1的卷积核来压缩通道数 |
| :----------------------------------------------------------: |
|  |
| **V2版本借鉴了VGG的理念（定制Inception时，在其内部采用标准化卷积核）** |
|  |
| **V3（2015）版本将VGG的理念发扬光大，将“标准化”推广到一般情况，并加入了BN** |
| |
| **V4（2016）版本在V3基础上选定了更合适的超参，没有引入残差的情况下，网络层数仍旧达到了76层** |

- **Inception-ResNet网络**

引入residual connection直连，把Inception和ResNet结合起来，让网络又宽又深

两个版本：结构基本相同，只是细节不同

- Inception-ResNet v1：Inception加ResNet，计算量和Inception v3相当，较小的模型
- Inception-ResNet v2：Inception加ResNet，计算量和Inception v4相当，较大的模型，准确率也更高





